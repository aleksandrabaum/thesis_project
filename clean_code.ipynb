{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests, re, subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acmg_url = 'https://www.ncbi.nlm.nih.gov/clinvar/docs/acmg/'\n",
    "\n",
    "response = requests.get(acmg_url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "results = soup.find(id=\"maincontent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_index = 2\n",
    "acmg_genes = []\n",
    "\n",
    "for row in results.find_all('tr'):\n",
    "    cell = row.find_all('td')\n",
    "\n",
    "    if len(cell) > column_index:\n",
    "        gene = cell[column_index].text.strip()\n",
    "        gene = re.sub(r'\\([^)]*\\)', '', gene).strip()\n",
    "        acmg_genes.append(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acmg_set = set(acmg_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_endpoint(server, request, content_type):\n",
    "\n",
    "    r = requests.get(server+request, headers={ \"Accept\" : content_type})\n",
    "\n",
    "    if not r.ok:\n",
    "        r.raise_for_status()\n",
    "        sys.exit()\n",
    "\n",
    "    if content_type == 'application/json':\n",
    "        return r.json()\n",
    "    else:\n",
    "        return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene(gene: str, grch: str='grch37'):\n",
    "\n",
    "    # get the Ensembl server and extension\n",
    "    if grch == 'grch37':\n",
    "        ens_serv: str = f'http://{grch}.rest.ensembl.org/'\n",
    "    elif grch == 'grch38':\n",
    "        ens_serv: str = f'http://rest.ensembl.org/'\n",
    "    else:\n",
    "        raise Exception('Please provide a correct grch assembly! (grch37/grch38 are supported)')\n",
    "    \n",
    "    #connect to server and get data\n",
    "    ens_ext: str = f'lookup/symbol/homo_sapiens/{gene}'\n",
    "    con: str = 'application/json'\n",
    "\n",
    "    return fetch_endpoint(ens_serv, ens_ext, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f'{wgs_mount}/regions.bed'\n",
    "os.system(f'touch {output_file}')\n",
    "with open(f'{output_file}', 'a') as f:\n",
    "    for idx, gene in reversed(list(enumerate(acmg_set))):\n",
    "        gene_json = get_gene(gene)\n",
    "        chromosome = gene_json['seq_region_name']\n",
    "        start = gene_json['start'] - 10000\n",
    "        end = gene_json['end'] + 10000\n",
    "        location = f'{chromosome}\\t{start}\\t{end}'\n",
    "        if idx > 0:\n",
    "            f.write(location)\n",
    "            f.write('\\n')\n",
    "        else:\n",
    "            f.write(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'bcftools view --regions-file {wgs_mount}/regions.bed -Oz {wgs_mount}/gnomad.genomes.r2.1.1.sites.vcf.bgz > {wgs_mount}/gnomad.genomes.regions.r2.1.1.sites.vcf.gz')\n",
    "os.system(f'bcftools index {wgs_mount}/gnomad.genomes.regions.r2.1.1.sites.vcf.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'bcftools query -f \"%CHROM %POS %ID %REF %ALT %QUAL %FILTER %INFO/AF\\n\" -o {wgs_mount}/extracted_gnomad.genomes.regions.r2.1.1.sites.vcf.gz {wgs_mount}/gnomad.genomes.regions.r2.1.1.sites.vcf.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = \"AF,MAX_AF,AFR_AF,AMR_AF,EAS_AF,EUR_AF,SAS_AF,Existing_variation,Allele,Consequence,Gene,NMD,SYMBOL,SYMBOL_SOURCE,IMPACT,ClinVar,ClinVar_CLNREVSTAT,ClinVar_AF_EXAC,ClinVar_CLNDISDB,ClinVar_CLNDISDBINCL,ClinVar_CLNSIG,ClinVar_CLNSIGCONF,ClinVar_CLNSIGINCL,ClinVar_CLNVC,ClinVar_GENEINFO,ClinVar_RS,gnomADg,gnomADg_AN,gnomADg_AC,gnomADg_AF,PUBMED,SIFT,PolyPhen,BIOTYPE,Feature_type,Feature,CLIN_SIG,MANE_SELECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete MANE Select - and then also from other places!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'docker run -i -v {wgs_mount}:/data ensemblorg/ensembl-vep vep \\\n",
    "--offline -i /data/gnomad_annotation/xai.vcf --assembly GRCh37 --vcf --af --max_af --af_1kg \\\n",
    "--fields {fields} --per_gene -o /data/out_xai.vcf --force_overwrite --mane --fork 5 --plugin NMD \\\n",
    "--custom /data/clinvar.vcf.gz,ClinVar,vcf,exact,0,CLNREVSTAT,AF_EXAC,CLNDISDB,CLNDISDBINCL,CLNSIG,CLNSIGCONF,CLNSIGINCL,CLNVC,GENEINFO,RS \\\n",
    "--custom file=/data/gnomad.genomes.regions.r2.1.1.sites.vcf.gz,short_name=gnomADg,format=vcf,type=exact,coords=0,fields=AN%AC%AF%AF_afr%AF_amr%AF_asj%AF_eas%AF_fin%AF_nfe%AF_oth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['CHROM','POS','ID','REF','ALT','QUAL','FILTER','INFO'] # INFO_AF ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('gnomad_annotation/annotated_mane/out_combined_below5.vcf', sep='\\t', names=column_names)\n",
    "df1['INFO'] = pd.to_numeric(df1['INFO'], errors='coerce')   # double check the INFO - it will probably had to be .split by ';' or sth\n",
    "print('df loaded')\n",
    "df_5 = df1[(df1['INFO'] <= 0.05) | (df1['INFO'].isna())] #(df1['AF'] == '-') | (df1['AF'].isna()) | (df1['AF'] == '.')]\n",
    "print('below 5 done')\n",
    "df_5.to_csv('gnomad_annotation/annotated_mane/below5_11.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clinvar = pd.read_csv(f'clinvar/variant_summary.txt', sep='\\t')\n",
    "\n",
    "df_clinvar = df_clinvar[df_clinvar['Assembly'] != 'GRCh38']\n",
    "df_clinvar['POS_ID'] = df_clinvar.apply(lambda row: f\"{row['Chromosome']}-{row['PositionVCF']}-{row['ReferenceAlleleVCF']}-{row['AlternateAlleleVCF']}\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gnom = pd.read_csv(f'gnomad_annotation/annotated_mane/out_gnomad_mane_add_annot_done.csv', sep=',', names=['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO'])\n",
    "df_gnom['POS_ID'] = df_gnom.apply(lambda row: f\"{row['CHROM']}-{row['POS']}-{row['REF']}-{row['ALT']}\", axis=1)\n",
    "merged_df = pd.merge(df_gnom, df_clinvar, on='POS_ID', how='left')\n",
    "merged_df.to_csv(f'gnomad_annotation/annotated_mane/clinvar_add_annot.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CHROM','POS','ID','REF','ALT','QUAL','FILTER','INFO','POS_ID','AlleleID','Type','Name','GeneID','GeneSymbol','HGNC_ID','ClinicalSignificance','ClinSigSimple','LastEvaluated','RS# (dbSNP)','nsv/esv (dbVar)','RCVaccession','PhenotypeIDS','PhenotypeList','Origin','OriginSimple','Assembly','ChromosomeAccession','Chromosome','Start','Stop','ReferenceAllele','AlternateAllele','Cytogenetic','ReviewStatus','NumberSubmitters','Guidelines','TestedInGTR','OtherIDs','SubmitterCategories','VariationID','PositionVCF','ReferenceAlleleVCF','AlternateAlleleVCF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clin = pd.read_csv(f'gnomad_annotation/annotated_mane/clinvar_add_annot.csv', sep=',', names=columns)\n",
    "final_clin_df = df_clin[df_clin['INFO'].str.contains('HIGH') | df_clin['ClinicalSignificance'].str.contains('Pathogenic') | df_clin['ClinicalSignificance'].str.contains('Pathogenic/Likely pathogenic') | df_clin['ClinicalSignificance'].str.contains('Likely pathogenic')]\n",
    "final_clin_df.to_csv(f'gnomad_annotation/annotated_mane/fin_clin_add_annot.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gnomad_annotation/combined_clin.csv', sep=',', names=columns)\n",
    "ids = df['ID']\n",
    "ids.to_csv('gnomad_annotation/rs_ids_combined_clin.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gnomad_annotation/rs_ids_combined_clin.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "filtered_lines = [line.strip() for line in lines if '.' not in line]\n",
    "\n",
    "with open('gnomad_annotation/rs_ids.txt', 'w') as file:\n",
    "    for line in filtered_lines:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gnomad_annotation/combined_clin.csv', sep=',', names=columns)\n",
    "filtered_df = df[df['ID'] == '.']\n",
    "filtered_values = filtered_df[['CHROM', 'POS']]\n",
    "filtered_values.to_csv('gnomad_annotation/test_rs_dots.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"bcftools view -i ID=@gnomad_annotation/rs_ids.txt gnomad_annotation/gnomad.genomes.regions.r2.1.1.sites.vcf.gz > gnomad_annotation/gnomad_zip_rows_tomatch.vcf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"bcftools view -R gnomad_annotation/test_rs_dots.txt gnomad_annotation/gnomad.genomes.regions.r2.1.1.sites.vcf.gz > gnomad_annotation/gnomad_zip_rows_tomatch_dots.vcf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_genotyped_file(path_to_file: str, separators=[\"\\t\", \",\"], possible_number_of_cols = [4,5],\n",
    "                        possible_header_starts = [\"RSID\", \"rsid\", \"#CHROM\", \"rsID\"]) -> pd.DataFrame:\n",
    "\n",
    "# first we handle the excell file\n",
    "    if path_to_file.endswith('xlsx'):\n",
    "        return pd.read_excel(path_to_file, engine='openpyxl')\n",
    "\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        header = None\n",
    "        skip_pattern = None\n",
    "        for line in f:\n",
    "            if line.startswith(\"[\"):\n",
    "                skip_pattern = True\n",
    "                break\n",
    "\n",
    "            if line.startswith('#'):\n",
    "                header = line\n",
    "            else:\n",
    "                if header is None:\n",
    "                    header = line\n",
    "                break # stop when there are no more\n",
    "    \n",
    "    if skip_pattern:\n",
    "        return pd.read_csv(path_to_file, sep=\"\\t\", skiprows=10)\n",
    "\n",
    "    is_header = any([(header_start in header) for header_start in possible_header_starts])\n",
    "\n",
    "    if is_header:\n",
    "\n",
    "        for separator in separators:\n",
    "            header_names = header[1:]\n",
    "            header_names = header_names.strip().split(separator)\n",
    "            if len(header_names)==1:\n",
    "                continue\n",
    "\n",
    "            data = pd.read_csv(path_to_file, sep=separator, comment='#', names=header_names)\n",
    "\n",
    "            if data.shape[1] in possible_number_of_cols:\n",
    "                break\n",
    "\n",
    "    else:\n",
    "\n",
    "        for separator in separators:\n",
    "            \n",
    "            data = pd.read_csv(path_to_file, sep=separator, comment='#')\n",
    "\n",
    "            if data.shape[1] in possible_number_of_cols:\n",
    "                break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gnomad = read_genotyped_file(f'gnomad_annotation/gnomad_zip_rows_tomatch.vcf')\n",
    "df_gnomad_2 = read_genotyped_file(f'gnomad_annotation/gnomad_zip_rows_tomatch_dots.vcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_gnomad, df_gnomad_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clin = pd.read_csv('gnomad_annotation/annotated_mane/combined_mane_clin.csv', sep=',', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(row, chosen_fields):\n",
    "    csq_info = row['CSQ'].split('CSQ=')[1]\n",
    "    csq_fields = csq_info.split('|')\n",
    "    # print(len(csq_fields))\n",
    "    chosen_fields = [csq_fields[i] if csq_fields[i] else '-' for i in chosen_fields]\n",
    "    return pd.Series(chosen_fields)\n",
    "\n",
    "def process_dataframe(df, chosen_indices, new_column_names):\n",
    "    new_columns = df.apply(extract_info, args=(chosen_indices,), axis=1)\n",
    "    new_columns.columns = new_column_names\n",
    "\n",
    "    # concatenate the new columns with the original df\n",
    "    df = pd.concat([df.loc[:, :'CSQ'], new_columns, df.loc[:, 'CSQ':]], axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('gnomad_annotation/annotated_mane/combined_mane_clin_test.xlsx')\n",
    "df = df[df['Assembly'] != 'GRCh38']\n",
    "\n",
    "chosen_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]\n",
    "new_column_names = ['AF','MAX_AF','AFR_AF','AMR_AF','EAS_AF','EUR_AF','SAS_AF','Existing_variation','Allele','Consequence','Gene','NMD','SYMBOL','SYMBOL_SOURCE','IMPACT','ClinVar','ClinVar_CLNREVSTAT','ClinVar_AF_EXAC','ClinVar_CLNDISDB','ClinVar_CLNDISDBINCL','ClinVar_CLNSIG','ClinVar_CLNSIGCONF','ClinVar_CLNSIGINCL','ClinVar_CLNVC','ClinVar_GENEINFO','ClinVar_RS','gnomADg','gnomADg_AN','gnomADg_AC','gnomADg_AF','PUBMED','SIFT','PolyPhen','BIOTYPE','Feature_type','Feature','CLIN_SIG','MANE_SELECT']\n",
    "\n",
    "# Process the dataframe to extract the new columns\n",
    "# new_columns, source_column_series = process_dataframe(df, chosen_indices, new_column_names)\n",
    "df_all = process_dataframe(df, chosen_indices, new_column_names)\n",
    "\n",
    "# Combine with original dataframe\n",
    "# df_combined = pd.concat([df.reset_index(drop=True), new_columns.reset_index(drop=True), source_column_series.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Filter out rows with 'NMD_escaping_variant' in NMD column\n",
    "# filtered_df = df_combined[df_combined['NMD'] != 'NMD_escaping_variant']\n",
    "filtered_df = df_all[df_all['NMD'] != 'NMD_escaping_variant']\n",
    "\n",
    "# Save to Excel\n",
    "filtered_df.to_excel('gnomad_annotation/annotated_mane/gnomad_mane_test_final_1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('gnomad_annotation/annotated_mane/gnomad_mane_test_final.xlsx')\n",
    "\n",
    "def extract_population_freq(info_string, population):\n",
    "    pattern = f\"AC_{population}=([0-9]+);AN_{population}=([0-9]+)\"\n",
    "    match = re.search(pattern, info_string)\n",
    "    if match:\n",
    "        ac, an = match.groups()\n",
    "        return float(ac), float(an)\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "populations = ['amr', 'asj', 'afr', 'eas', 'fin', 'nfe_nwe', 'nfe_seu', 'nfe_onf', 'nfe_est']   # 'nfe'\n",
    "for population in populations:\n",
    "    ac_values = []\n",
    "    an_values = []\n",
    "    for index, row in df.iterrows():\n",
    "        ac, an = extract_population_freq(row['INFO'], population)   # CHECK LATER IF IT SHOULDN'T BE CHANGED INTO INFO_x\n",
    "        ac_values.append(ac)\n",
    "        an_values.append(an)\n",
    "    df[f'AC_{population}'] = ac_values\n",
    "    df[f'AN_{population}'] = an_values\n",
    "    \n",
    "for population in populations:\n",
    "    df[f'Frequency_{population}'] = df[f'AC_{population}'] / df[f'AN_{population}']\n",
    "df.to_excel('gnomad_annotation/annotated_mane/gnomad_mane_populations_final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('gnomad_mane_populations_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the cleaned DataFrame to a new .xlsx file\n",
    "output_path = 'cleaned_file.xlsx'  # Replace with your desired output file path\n",
    "df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('cleaned_file.xlsx')\n",
    "\n",
    "# Remove duplicate rows based on the POS_ID column\n",
    "df_cleaned = df.drop_duplicates(subset=['POS_ID'])\n",
    "\n",
    "# Save the cleaned DataFrame to a new .xlsx file\n",
    "output_path = 'cleaned_file_posid_dups.xlsx'  # Replace with your desired output file path\n",
    "df_cleaned.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one of the drop_duplicates in the final clean version !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('cleaned_file_posid_dups.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'cleaned_file_posid_dups.xlsx'  # Replace with your actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Remove duplicate rows based on the POS_ID column\n",
    "# df_cleaned = df.drop_duplicates(subset=['POS_ID'])\n",
    "\n",
    "# Filter rows where SubmitterCategories is 2 or more, or is NaN\n",
    "df_filtered = df[(df['NumberSubmitters'] >= 2) | (df['NumberSubmitters'].isna())]\n",
    "\n",
    "df_filtered_ag = df_filtered[df_filtered['ClinicalSignificance'] != 'Conflicting interpretations of pathogenicity']\n",
    "\n",
    "# Save the filtered DataFrame to a new .xlsx file\n",
    "output_path = 'filtered_more_submits.xlsx'  # Replace with your desired output file path\n",
    "df_filtered_ag.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('filtered_more_submits.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'filtered_more_submits.xlsx'  # Replace with your actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "lst = ['Pathogenic','Pathogenic/Likely pathogenic','Likely pathogenic','Pathogenic; drug response','Likely pathogenic; drug response']\n",
    "# Remove duplicate rows based on the POS_ID column\n",
    "# df_cleaned = df.drop_duplicates(subset=['POS_ID'])\n",
    "\n",
    "# Filter rows where SubmitterCategories is 2 or more, or is NaN\n",
    "# df_filtered = df[(df['NumberSubmitters'] >= 2) | (df['NumberSubmitters'].isna())]\n",
    "\n",
    "df_filtered_ag = df_filtered.loc[(df_filtered['ClinicalSignificance'].isin(lst)) | (df_filtered['ClinicalSignificance'].isna()),]\n",
    "\n",
    "# Save the filtered DataFrame to a new .xlsx file\n",
    "output_path = 'filtered_more_significance.xlsx'  # Replace with your desired output file path\n",
    "df_filtered_ag.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('filtered_more_significance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'filtered_more_submits.xlsx'  # Replace with your actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "lst = ['Pathogenic','Pathogenic/Likely pathogenic','Likely pathogenic','Pathogenic; drug response','Likely pathogenic; drug response']\n",
    "# Remove duplicate rows based on the POS_ID column\n",
    "# df_cleaned = df.drop_duplicates(subset=['POS_ID'])\n",
    "\n",
    "# Filter rows where SubmitterCategories is 2 or more, or is NaN\n",
    "# df_filtered = df[(df['NumberSubmitters'] >= 2) | (df['NumberSubmitters'].isna())]\n",
    "\n",
    "df_filtered_ag = df_filtered.loc[(df_filtered['ClinicalSignificance'].isin(lst)),]\n",
    "\n",
    "# Save the filtered DataFrame to a new .xlsx file\n",
    "output_path = 'filtered_more_significance_test.xlsx'  # Replace with your desired output file path\n",
    "df_filtered_ag.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('filtered_more_significance_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('SYMBOL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_columns = ['AN_amr', 'AN_asj', 'AN_afr', 'AN_eas', 'AN_fin', 'AN_nfe_nwe', 'AN_nfe_seu', 'AN_nfe_est', 'AN_nfe_onf']  # 'AN_nfe'\n",
    "ac_columns = ['AC_amr', 'AC_asj', 'AC_afr', 'AC_eas', 'AC_fin', 'AC_nfe_nwe', 'AC_nfe_seu', 'AC_nfe_est', 'AC_nfe_onf']  # 'AC_nfe'\n",
    "\n",
    "# symbols = ['PKP2', 'BTD', 'MLH1']\n",
    "\n",
    "# Initialize dictionaries to store median and mode results\n",
    "median_results_an = {}\n",
    "mode_results_an = {}\n",
    "# median_results_ac = {}\n",
    "# mode_results_ac = {}\n",
    "sums_ac = {}\n",
    "ratios_mode = {}\n",
    "ratios_median = {}\n",
    "\n",
    "# Iterate over each AN column\n",
    "for column in an_columns:\n",
    "    # Calculate the median of the current AN column within each group\n",
    "    median_results_an[column] = df.groupby('SYMBOL')[column].median().reset_index()\n",
    "    \n",
    "    # Calculate the mode of the current AN column within each group\n",
    "    # Note: Mode can have multiple values, so we'll use a lambda function to join them into a single string\n",
    "    mode_results_an[column] = df.groupby('SYMBOL')[column].agg(lambda x: ','.join(x.mode().astype(str))).reset_index()\n",
    "\n",
    "for column in ac_columns:\n",
    "    # Calculate the median of the current AN column within each group\n",
    "    # median_results_ac[column] = df.groupby('SYMBOL')[column].median().reset_index()\n",
    "    \n",
    "    # Calculate the mode of the current AN column within each group\n",
    "    # Note: Mode can have multiple values, so we'll use a lambda function to join them into a single string\n",
    "    # mode_results_ac[column] = df.groupby('SYMBOL')[column].agg(lambda x: ','.join(x.mode().astype(str))).reset_index()\n",
    "\n",
    "    sums_ac[column] = df.groupby('SYMBOL')[column].sum().reset_index()\n",
    "\n",
    "print('good')\n",
    "results_median = []\n",
    "results_mode = []\n",
    "\n",
    "for key in sums_ac.keys():\n",
    "    ac_df = sums_ac[key]\n",
    "    an_df = median_results_an[key.replace(\"AC\", \"AN\")]\n",
    "    \n",
    "    # Merge the DataFrames on 'SYMBOL' column\n",
    "    merged_df = pd.merge(ac_df, an_df, on='SYMBOL')\n",
    "    \n",
    "    # Perform the division operation\n",
    "    merged_df[f'AC/AN_ratio_{key}'] = merged_df[key] / merged_df[key.replace(\"AC\", \"AN\")]\n",
    "    \n",
    "    # Append the result to the list\n",
    "    results_median.append(merged_df)\n",
    "print('good')\n",
    "\n",
    "for key in sums_ac.keys():\n",
    "    # print(key)\n",
    "    ac_df = sums_ac[key]\n",
    "    # print(ac_df)\n",
    "    an_df = mode_results_an[key.replace(\"AC\", \"AN\")]\n",
    "\n",
    "    # Function to find the maximum value from a list of strings\n",
    "    # def max_from_list(lst):\n",
    "    #     if isinstance(lst, list):\n",
    "    #         return max(map(float, lst))\n",
    "    #     else:\n",
    "    #         return float(lst)\n",
    "\n",
    "    # print(ac_df)\n",
    "\n",
    "    # ac_df[key] = ac_df[key].apply(lambda x: max(map(int, x.split(','))) if ',' in x else int(x))\n",
    "    an_df[key.replace(\"AC\", \"AN\")] = an_df[key.replace(\"AC\", \"AN\")].apply(lambda x: max(map(int, x.split(','))) if ',' in x else int(x))\n",
    "\n",
    "    # an_max = an_df.apply(lambda x: max_from_list(x.split(',')), axis=1)\n",
    "    # ac_max = ac_df.apply(lambda x: max_from_list(x.split(',')), axis=1)\n",
    "\n",
    "    # print(ac_max)\n",
    "\n",
    "    # print(an_df)\n",
    "    # Merge the DataFrames on 'SYMBOL' column\n",
    "    merged_df = pd.merge(ac_df, an_df, on='SYMBOL')\n",
    "    \n",
    "    # Perform the division operation\n",
    "    merged_df[f'AC/AN_ratio_{key}'] = merged_df[key] / merged_df[key.replace(\"AC\", \"AN\")]\n",
    "    \n",
    "    # Append the result to the list\n",
    "    results_mode.append(merged_df)\n",
    "print('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_median = {}\n",
    "\n",
    "for df in results_median:\n",
    "    for column in df.columns:\n",
    "        if column.startswith('AN_'):\n",
    "            column = column.lstrip('AN_')\n",
    "            df_dict_median[column] = df\n",
    "            break\n",
    "\n",
    "df_dict_mode = {}\n",
    "\n",
    "for df in results_mode:\n",
    "    for column in df.columns:\n",
    "        if column.startswith('AN_'):\n",
    "            column = column.lstrip('AN_')\n",
    "            df_dict_mode[column] = df\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_names_pop = {'afr':'African','amr':'Latino','asj':'Ashkenazi Jews', 'eas':'East Asian','fin':'Finnish','nfe_nwe':'North-Western European','nfe_seu':'Southern European','nfe_onf':'Other Non-Finnish European','nfe_est':'Estonian'} # 'nfe':'Non-Finnish European'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_info = {}\n",
    "\n",
    "for population, df in df_dict_median.items():\n",
    "    filtered_df = df[df['SYMBOL'].isin(acmg_genes)]\n",
    "    # Extract relevant columns from the dataframe\n",
    "    population_df = filtered_df[['SYMBOL', f'AC_{population}', f'AN_{population}']]\n",
    "\n",
    "    population_df[f'AN-AC_{population}'] = filtered_df[f'AN_{population}'] - filtered_df[f'AC_{population}']\n",
    "    \n",
    "    # Convert the dataframe to a list of dictionaries\n",
    "    population_list = population_df.to_dict(orient='records')\n",
    "    \n",
    "    # Store the list of dictionaries in the population_info dictionary\n",
    "    population_info[population] = population_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_results = {}\n",
    "\n",
    "# Iterate through each SYMBOL\n",
    "for symbol in acmg_genes:\n",
    "    if symbol not in [data_dict['SYMBOL'] for data_list in population_info.values() for data_dict in data_list]:\n",
    "        continue  # Skip the symbol if it has no entries\n",
    "\n",
    "    # Initialize an empty dictionary to store results for the current symbol\n",
    "    symbol_results = {}\n",
    "    \n",
    "    # Generate all pairs of populations\n",
    "    population_pairs = combinations(population_info.keys(), 2)\n",
    "    \n",
    "    # Iterate through each pair of populations\n",
    "    for population_a, population_b in population_pairs:\n",
    "        # Extract data for the current populations\n",
    "        data_a = population_info[population_a]\n",
    "        data_b = population_info[population_b]\n",
    "        \n",
    "        # Initialize contingency table\n",
    "        contingency_table = [[], []]\n",
    "        \n",
    "        # Fill contingency table with AC and AN-AC values for the current symbol\n",
    "        for data_dict_a, data_dict_b in zip(data_a, data_b):\n",
    "            if data_dict_a['SYMBOL'] == symbol and data_dict_b['SYMBOL'] == symbol:\n",
    "                # print(symbol)\n",
    "                contingency_table[0].append(data_dict_a[f'AC_{population_a}'])\n",
    "                contingency_table[1].append(data_dict_b[f'AC_{population_b}'])\n",
    "                contingency_table[0].append(data_dict_a[f'AN-AC_{population_a}'])\n",
    "                contingency_table[1].append(data_dict_b[f'AN-AC_{population_b}'])\n",
    "        \n",
    "        # print(symbol, population_a, population_b)\n",
    "        # if symbol == '':\n",
    "        #     print(population_a, population_b)\n",
    "        # if len(contingency_table) == '':\n",
    "        #     print(population_a, population_b)\n",
    "        \n",
    "        # Perform Fisher's exact test\n",
    "        odds_ratio, p_value = fisher_exact(contingency_table)\n",
    "        \n",
    "        # Store results for the current pair of populations\n",
    "        symbol_results[(population_a, population_b)] = {'odds_ratio': odds_ratio, 'p_value': p_value}\n",
    "    \n",
    "    # Store Fisher's exact test results for the current symbol\n",
    "    fisher_results[symbol] = symbol_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dict = {}\n",
    "\n",
    "# Open the text file\n",
    "with open('genes phenotypes groups.txt', 'r') as file:\n",
    "    current_group = None\n",
    "    for line in file:\n",
    "        # Remove leading and trailing whitespace from the line\n",
    "        line = line.strip()\n",
    "        # Check if the line is empty\n",
    "        if not line:\n",
    "            # If the line is empty, it indicates a new group\n",
    "            current_group = None\n",
    "        else:\n",
    "            # If the line is not empty\n",
    "            if current_group is None:\n",
    "                # If current_group is None, it indicates a new group\n",
    "                current_group = line\n",
    "                # Initialize a list for the current group\n",
    "                groups_dict[current_group] = []\n",
    "            else:\n",
    "                # If current_group is not None, append the symbol to the list of the current group\n",
    "                groups_dict[current_group].append(line)\n",
    "\n",
    "# Display the dictionary\n",
    "print(groups_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped_dict = {}\n",
    "\n",
    "for population, df in df_dict_median.items():\n",
    "    # Initialize dictionaries to store sums of AC and medians of AN for each group\n",
    "    ac_sums = {}\n",
    "    ac_minus_an_median = {}\n",
    "    \n",
    "    # Iterate through each group of symbols\n",
    "    for group, symbols in groups_dict.items():\n",
    "        # Filter the DataFrame to include only symbols in the current group\n",
    "        group_df = df[df['SYMBOL'].isin(symbols)]\n",
    "        \n",
    "        # Calculate the sum of AC values for the current group\n",
    "        ac_sum = group_df[f'AC_{population}'].sum()\n",
    "        ac_sums[group] = ac_sum\n",
    "        \n",
    "        # Calculate the median of AN values for the current group\n",
    "        # an_median = group_df[f'AN_{population}'].median()\n",
    "        ac_minus_an_median[group] = group_df[f'AN_{population}'].median() - group_df[f'AC_{population}'].sum()\n",
    "        # an_medians[group] = ac_minus_an_median\n",
    "    \n",
    "    # Store the results for the current population\n",
    "    results_grouped_dict[population] = {'AC_sums': ac_sums, f'AN-AC_{population}': ac_minus_an_median}\n",
    "\n",
    "# Display the results\n",
    "print(results_grouped_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_results_grouped = {}\n",
    "\n",
    "# Iterate through each group\n",
    "for group_name in list(results_grouped_dict[next(iter(results_grouped_dict))]['AC_sums'].keys()):\n",
    "    fisher_results_grouped[group_name] = {}\n",
    "    \n",
    "    # Iterate through each pair of populations\n",
    "    for population_pair in combinations(results_grouped_dict.keys(), 2):\n",
    "        population_a, population_b = population_pair\n",
    "        \n",
    "        # Extract AC_sums and AN-AC values for population A\n",
    "        ac_sum_a = results_grouped_dict[population_a]['AC_sums'][group_name]\n",
    "        an_minus_ac_a = results_grouped_dict[population_a]['AN-AC_' + population_a][group_name]\n",
    "        \n",
    "        # Extract AC_sums and AN-AC values for population B\n",
    "        ac_sum_b = results_grouped_dict[population_b]['AC_sums'][group_name]\n",
    "        an_minus_ac_b = results_grouped_dict[population_b]['AN-AC_' + population_b][group_name]\n",
    "        \n",
    "        # Construct the contingency table\n",
    "        contingency_table = [\n",
    "            [ac_sum_a, an_minus_ac_a],\n",
    "            [ac_sum_b, an_minus_ac_b]\n",
    "        ]\n",
    "        \n",
    "        # Perform Fisher's exact test\n",
    "        odds_ratio, p_value = fisher_exact(contingency_table)\n",
    "        \n",
    "        # Store the results\n",
    "        fisher_results_grouped[group_name][population_pair] = {'odds_ratio': odds_ratio, 'p_value': p_value}\n",
    "\n",
    "# Display the Fisher's exact test results\n",
    "print(fisher_results_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in acmg_genes:\n",
    "    if symbol in fisher_results.keys():\n",
    "        total_ac_an_ratio = 0\n",
    "        for column, median_df in df_dict_median.items():\n",
    "            pop = column\n",
    "            symbol_median_df = median_df[median_df['SYMBOL'] == symbol]\n",
    "            if not symbol_median_df.empty:\n",
    "                total_ac_an_ratio += symbol_median_df[f'AC/AN_ratio_AC_{pop}'].iloc[0]\n",
    "\n",
    "        # Skip plotting if total AC/AN ratio is zero\n",
    "        if total_ac_an_ratio == 0:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(15, 17))\n",
    "        plt.title(f'Frequency of individuals with a variant present in {symbol} gene', fontsize=20)\n",
    "        plt.xlabel('Populations', fontsize=16)\n",
    "        plt.ylabel('Frequency', fontsize=16)\n",
    "\n",
    "        for i, (column, median_df) in enumerate(df_dict_median.items()):\n",
    "            pop = column\n",
    "            symbol_median_df = median_df[median_df['SYMBOL'] == symbol]\n",
    "            plt.bar(i, symbol_median_df['AC/AN_ratio_AC_' + pop], alpha=0.8, label=f'{column}')\n",
    "\n",
    "        # Adding horizontal lines representing p-values between different population pairs\n",
    "        for i, ((pop1, pop2), p_value_data) in enumerate(fisher_results[symbol].items()):\n",
    "            if p_value_data['p_value'] < 0.05:\n",
    "                p_value = p_value_data['p_value']\n",
    "                x1 = list(df_dict_median.keys()).index(pop1)\n",
    "                x2 = list(df_dict_median.keys()).index(pop2)\n",
    "                y = max(plt.ylim())\n",
    "                plt.plot([x1, x2], [y, y], linewidth=2, color='black')  # Horizontal line\n",
    "                p_value_label = f'{p_value:.4f}' if p_value >= 1e-4 else f'<{0.0001}'\n",
    "                plt.text((x1 + x2) / 2, y, f'p-value: {p_value_label}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "        plt.xticks(range(len(df_dict_median)), [full_names_pop[pop] for pop in df_dict_median.keys()], rotation=-45, fontsize=12)\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to fit all elements\n",
    "        plt.savefig(f'mane_plots_significance/freq_fisher/{symbol}.jpg')\n",
    "        # plt.savefig(f'mane_plots_significance_test/freq_fisher/{symbol}.jpg')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_ratios = {}\n",
    "\n",
    "for disease, symbols in groups_dict.items():\n",
    "    disease_df = pd.DataFrame()\n",
    "    for symbol in symbols:\n",
    "        symbol_data = pd.concat([data[data['SYMBOL'] == symbol] for _, data in df_dict_median.items()])\n",
    "        # print(symbol_data)\n",
    "        disease_df = pd.concat([disease_df, symbol_data])\n",
    "    \n",
    "    # Calculate sum of AC values and median of AN values for each population\n",
    "    disease_ratios[disease] = {}\n",
    "    for population, data in df_dict_median.items():\n",
    "        # print(population)\n",
    "        population_subset = disease_df[disease_df['SYMBOL'].isin(symbols)]\n",
    "        ac_sum = population_subset[f'AC_{population}'].sum()\n",
    "        an_median = population_subset[f'AN_{population}'].median()\n",
    "        # Calculate new AC/AN ratio\n",
    "        if an_median != 0:\n",
    "            ratio = ac_sum / an_median\n",
    "        else:\n",
    "            ratio = None  # Handle case where denominator is 0\n",
    "        disease_ratios[disease][population] = ratio\n",
    "\n",
    "print(disease_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disease, ratios_per_population in disease_ratios.items():\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    \n",
    "    # Title and labels with font size adjustments\n",
    "    plt.title(f'Frequency of individuals with a variant present in genes\\nrelated to {disease}', fontsize=20, pad=20)\n",
    "    plt.xlabel('Populations', fontsize=16)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "\n",
    "    for i, (population, ratio) in enumerate(ratios_per_population.items()):\n",
    "        plt.bar(i, ratio, alpha=0.8, label=f'{population}')\n",
    "    \n",
    "    # Adding horizontal lines representing p-values between different population pairs\n",
    "    if disease in fisher_results_grouped:\n",
    "        for i, ((pop1, pop2), p_value_data) in enumerate(fisher_results_grouped[disease].items()):\n",
    "            if p_value_data['p_value'] < 0.05:\n",
    "                p_value = p_value_data['p_value']\n",
    "                x1 = list(ratios_per_population.keys()).index(pop1)\n",
    "                x2 = list(ratios_per_population.keys()).index(pop2)\n",
    "                y = max(plt.ylim())\n",
    "                plt.plot([x1, x2], [y, y], linewidth=2, color='black')  # Horizontal line\n",
    "                p_value_label = f'{p_value:.4f}' if p_value >= 1e-4 else f'<{0.0001}'\n",
    "                plt.text((x1 + x2) / 2, y, f'p-value: {p_value_label}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.xticks(range(len(ratios_per_population)), [full_names_pop[pop] for pop in ratios_per_population.keys()], rotation=-45, fontsize=12)\n",
    "\n",
    "    # Adjust layout to fit all elements\n",
    "    plt.tight_layout()  # Adjust rect to leave more space for the title\n",
    "\n",
    "    plt.savefig(f'mane_plots_significance/freq_fisher_grouped/{disease}.jpg')\n",
    "    # plt.savefig(f'mane_plots_significance_test/freq_fisher_grouped/{disease}.jpg')\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
